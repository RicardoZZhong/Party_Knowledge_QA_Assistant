{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1885d3a7-069e-46a9-ac0a-2e919cc697db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T05:51:38.015671Z",
     "iopub.status.busy": "2024-10-17T05:51:38.015363Z",
     "iopub.status.idle": "2024-10-17T05:51:43.516113Z",
     "shell.execute_reply": "2024-10-17T05:51:43.515592Z",
     "shell.execute_reply.started": "2024-10-17T05:51:38.015643Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# 向量模型下载\n",
    "from modelscope import snapshot_download\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "from typing import Any, List, Optional\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba1a835-c630-4a44-b793-f98593da138b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T05:51:43.517362Z",
     "iopub.status.busy": "2024-10-17T05:51:43.517026Z",
     "iopub.status.idle": "2024-10-17T05:51:54.057999Z",
     "shell.execute_reply": "2024-10-17T05:51:54.057548Z",
     "shell.execute_reply.started": "2024-10-17T05:51:43.517344Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading [added_tokens.json]: 100%|██████████| 82.0/82.0 [00:00<00:00, 195B/s]\n",
      "Downloading [config.json]: 100%|██████████| 871/871 [00:00<00:00, 1.74kB/s]\n",
      "Downloading [configuration.json]: 100%|██████████| 47.0/47.0 [00:00<00:00, 86.1B/s]\n",
      "Downloading [pytorch_model.bin]: 100%|██████████| 636M/636M [00:04<00:00, 152MB/s]     \n",
      "Downloading [README.md]: 100%|██████████| 24.8k/24.8k [00:00<00:00, 59.8kB/s]\n",
      "Downloading [special_tokens_map.json]: 100%|██████████| 125/125 [00:00<00:00, 247B/s]\n",
      "Downloading [tokenizer.json]: 100%|██████████| 429k/429k [00:00<00:00, 620kB/s]  \n",
      "Downloading [tokenizer_config.json]: 100%|██████████| 1.08k/1.08k [00:00<00:00, 2.03kB/s]\n",
      "Downloading [vocab.txt]: 100%|██████████| 107k/107k [00:00<00:00, 188kB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./AI-ModelScope/tao-8k'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download('AI-ModelScope/tao-8k', cache_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40109f34-3782-4d02-8cf0-417248403843",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-17T05:58:56.997144Z",
     "iopub.status.busy": "2024-10-17T05:58:56.996742Z",
     "iopub.status.idle": "2024-10-17T06:04:57.785073Z",
     "shell.execute_reply": "2024-10-17T06:04:57.784569Z",
     "shell.execute_reply.started": "2024-10-17T05:58:56.997116Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./AI-ModelScope/tao-8k. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n",
      "536\n",
      "542\n",
      "427\n",
      "538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_374/1186819388.py:68: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "def get_embedding(embedding_model_path):\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}  # set True to compute cosine similarity\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "def get_vectordb(file_path: str=None, persist_path: str=None, embedding_path=None):\n",
    "    \"\"\"\n",
    "    返回向量数据库对象\n",
    "    输入参数：\n",
    "    question：\n",
    "    llm:\n",
    "    vectordb:向量数据库(必要参数),一个对象\n",
    "    template：提示模版（可选参数）可以自己设计一个提示模版，也有默认使用的\n",
    "    embedding：可以使用zhipuai等embedding，不输入该参数则默认使用 openai embedding，注意此时api_key不要输错\n",
    "    \"\"\"\n",
    "\n",
    "    embedding = get_embedding(embedding_path)\n",
    "    if os.path.exists(persist_path):  #持久化目录存在\n",
    "        contents = os.listdir(persist_path)\n",
    "        if len(contents) == 0:  #但是下面为空\n",
    "            #print(\"目录为空\")\n",
    "            vectordb = create_db(file_path, persist_path, embedding)\n",
    "        else:\n",
    "            #print(\"目录不为空\")\n",
    "            vectordb = load_knowledge_db(persist_path, embedding)\n",
    "    else: #目录不存在，从头开始创建向量数据库\n",
    "        vectordb = create_db(file_path, persist_path, embedding)\n",
    "        #presit_knowledge_db(vectordb)\n",
    "        # vectordb = load_knowledge_db(persist_path, embedding)\n",
    "\n",
    "    return vectordb\n",
    "\n",
    "def create_db(embedding_model, file_path:str=None, persist_path:str=None):\n",
    "\n",
    "    loaders_chinese = [\n",
    "        PyPDFLoader(\"./knowledge/中国近现代史纲要：2023 年版(1).pdf\"),\n",
    "        PyPDFLoader(\"./knowledge/《习近平谈治国理政》第一卷.pdf\"),\n",
    "        PyPDFLoader(\"./knowledge/《习近平谈治国理政》第二卷.pdf\"),\n",
    "        PyPDFLoader(\"./knowledge/《习近平谈治国理政》第三卷.pdf\"),\n",
    "        PyPDFLoader(\"./knowledge/《习近平谈治国理政》第四卷.pdf\")\n",
    "        # 如果需要还可以加入其他文件\n",
    "    ]\n",
    "    docs = []\n",
    "    for loader in loaders_chinese:\n",
    "        pages = loader.load()\n",
    "        print(len(pages))\n",
    "        docs.extend(pages)\n",
    "    CHUNK_SIZE = 200\n",
    "    # 知识库中相邻文本重合长度\n",
    "    OVERLAP_SIZE = 50\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=OVERLAP_SIZE\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_path  # 允许我们将persist_directory目录保存到磁盘上\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def load_knowledge_db(persist_path, embedding):\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_path,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "\n",
    "tao_8k_embedding_model_path = \"./AI-ModelScope/tao-8k\"\n",
    "tao_8k = get_embedding(tao_8k_embedding_model_path)\n",
    "vectordb = create_db(tao_8k, persist_path=\"./vector_db/tao_8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122aadd1-49bc-4287-9824-4f516c608845",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-10-17T06:14:54.678490Z",
     "iopub.status.busy": "2024-10-17T06:14:54.677923Z",
     "iopub.status.idle": "2024-10-17T06:14:54.773915Z",
     "shell.execute_reply": "2024-10-17T06:14:54.773388Z",
     "shell.execute_reply.started": "2024-10-17T06:14:54.678467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "# 继承自 langchain_core.language_models.llms.LLM,用于调用 ZhipuAI 的语言模型服务\n",
    "class ZhipuAILLM(LLM):\n",
    "    # 默认选用 glm-4 模型\n",
    "    model: str = \"glm-4\"\n",
    "    # 温度系数\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"652a160546149ef4e3ec0ff881beebfe.D3UaKuk7FmiUn9WQ\"\n",
    "    max_tokens: int = 2048\n",
    "\n",
    "    # 定义 _call 方法：\n",
    "    # 这个方法实现了实际的 API 调用逻辑：\n",
    "    # 初始化 ZhipuAI 客户端。\n",
    "    # 生成请求参数messages。\n",
    "    # 调用 chat.completions.create 方法获取响应。\n",
    "    # 返回响应中的内容，如果没有结果则返回错误信息。\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # 生成 GLM 模型请求参数的方法：\n",
    "        # 生成 GLM 模型的请求参数 messages，包括系统消息和用户输入\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            构造 GLM 模型请求参数 messages\n",
    "            请求参数：\n",
    "                prompt: 对应的用户提示词\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "\n",
    "        client = ZhipuAI(\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "\n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "    # 定义属性方法：\n",
    "    # _default_params：返回调用 API 的默认参数。\n",
    "    # _llm_type：返回模型类型的字符串标识。\n",
    "    # _identifying_params：返回模型的标识参数。\n",
    "    # 首先定义一个返回默认参数的方法\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55bc5b-53fc-4177-8552-23fe891ac7d1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "# 导入PromptTemplate类，用于创建问题模板\n",
    "from langchain.chains import RetrievalQA\n",
    "# 导入RetrievalQA类，用于执行检索问答\n",
    "import sys\n",
    "# 导入sys模块，用于操作系统相关功能\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "# 添加上级目录到模块搜索路径\n",
    "# from qa_chain.model_to_llm import model_to_llm\n",
    "# # 导入model_to_llm函数，用于将模型转换为大语言模型(LLM)\n",
    "# from qa_chain.get_vectordb import get_vectordb\n",
    "# # 导入get_vectordb函数，用于获取向量数据库\n",
    "\n",
    "# 定义 QA_chain_self 类：\n",
    "# 该类用于创建和管理一个问答链系统，不带历史记录，类的描述文档说明了各个参数的用途。\n",
    "class QA_chain_self():\n",
    "    # 类描述\n",
    "    \"\"\"\n",
    "    不带历史记录的问答链\n",
    "    - model：调用的模型名称\n",
    "    - temperature：温度系数，控制生成的随机性\n",
    "    - top_k：返回检索的前k个相似文档\n",
    "    - file_path：建库文件所在路径\n",
    "    - persist_path：向量数据库持久化路径\n",
    "    - api_key：所有模型都需要\n",
    "    - embeddings：使用的embedding模型\n",
    "    - embedding_key：使用的embedding模型的秘钥（智谱或者OpenAI）\n",
    "    - template：可以自定义提示模板，没有输入则使用默认的提示模板default_template_rq\n",
    "    \"\"\"\n",
    "\n",
    "    # 默认的提示模板，用于构建问答的输入\n",
    "    default_template_rq = \"\"\"\n",
    "    使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。最多使用三句话。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。\n",
    "    {context}\n",
    "    问题: {question}\n",
    "    有用的回答:\"\"\"\n",
    "\n",
    "    template1 = \"\"\"给你一个任务：给定一段文本，这段文本中第一段是题目，题目中会有括号，后面四行为A，B，C，D四个选项，最后一行是题目的答案。你需要做的是：将题目中的括号以及括号内的内容替换为答案并输出问题，注意不需要输出选项。\n",
    "    不需要你检查指出题目的对错，不要输出除了以上任务之外的任何回答。\n",
    "    这是你需要处理的文本：{question}\n",
    "    \"\"\"\n",
    "    template2 = \"\"\"使用以下背景知识来回答最后的问题。不要试图编造答案。尽量简明扼要地回答。\n",
    "    背景知识：{context}\n",
    "    问题：{query}\"\"\"\n",
    "\n",
    "\n",
    "    # 构造函数，初始化类实例\n",
    "    # 初始化向量数据库和LLM\n",
    "    # 初始化 PromptTemplate 和 RetrievalQA 类\n",
    "    def __init__(self, model: str, temperature: float = 0.0, top_k: int = 4, choiceproblem_file_path: str = None,\n",
    "                 analysis_vectordb_file_path: str = None, persist_path: str = None, api_key: str = None,\n",
    "                 embedding=\"zhipuai\", embedding_path=None, template=default_template_rq):\n",
    "        # 类属性初始化\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        # self.file_path = file_path\n",
    "        self.persist_path = persist_path\n",
    "        self.api_key = api_key\n",
    "        self.embedding = embedding\n",
    "        # self.embedding_key = embedding_key\n",
    "        self.embedding_path = embedding_path\n",
    "        self.template = template\n",
    "        client = ZhipuAI(api_key=\"652a160546149ef4e3ec0ff881beebfe.D3UaKuk7FmiUn9WQ\")\n",
    "        # 加载已存在的向量数据库\n",
    "        # self.choiceproblem_vectordb = get_vectordb(choiceproblem_file_path, persist_path + \"/choiceproblem\", embedding_path)\n",
    "        self.analysis_vectordb = get_vectordb(analysis_vectordb_file_path, persist_path + \"/tao_8k\", embedding_path)\n",
    "        self.llm = ZhipuAILLM()\n",
    "\n",
    "        # # 初始化PromptTemplate和RetrievalQA类\n",
    "        # self.QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=self.template)\n",
    "        # self.retriever = self.vectordb.as_retriever(search_type=\"similarity\", search_kwargs={'k': self.top_k})\n",
    "        # self.qa_chain = RetrievalQA.from_chain_type(llm=self.llm, retriever=self.retriever,\n",
    "        #                                             return_source_documents=True,\n",
    "        #                                             chain_type_kwargs={\"prompt\": self.QA_CHAIN_PROMPT})\n",
    "        self.prompt1 = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=self.template1\n",
    "        )\n",
    "        self.chain1 = LLMChain(\n",
    "            llm=self.llm, prompt=self.prompt1\n",
    "        )\n",
    "\n",
    "        self.prompt2 = PromptTemplate(\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "            template=self.template2\n",
    "        )\n",
    "        self.chain2 = LLMChain(\n",
    "            llm=self.llm, prompt=self.prompt2\n",
    "        )\n",
    "\n",
    "\n",
    "    # 提供问答功能的方法，根据用户问题调用问答链并返回结果\n",
    "    def answer(self, query: str = None, temperature=None, top_k=4):\n",
    "        \"\"\"\n",
    "        核心方法，调用问答链\n",
    "        arguments:\n",
    "        - question：用户提问\n",
    "        \"\"\"\n",
    "        # 检查问题是否为空\n",
    "        if len(query) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        # 如果没有指定温度或top_k参数，则使用初始化时的值\n",
    "        if temperature is None:\n",
    "            temperature = self.temperature\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k\n",
    "\n",
    "        # sim_docs = self.choiceproblem_vectordb.similarity_search(query, k=1)\n",
    "        # question = \"\"\n",
    "        # for sim_doc in sim_docs:\n",
    "        #     print(sim_doc.page_content)\n",
    "        #     print(\"--------------\")\n",
    "        #     question = question + sim_doc.page_content+'\\n'\n",
    "        # response = self.chain1(question)\n",
    "        # print(response)\n",
    "        # print(\"---------\\n\")\n",
    "        # context = response['text']\n",
    "        context = \"\"\n",
    "        related_analysis = self.analysis_vectordb.similarity_search(query, k=3)\n",
    "        for related_doc in related_analysis:\n",
    "            # print(type(related_doc))\n",
    "            context = context + related_doc.page_content\n",
    "        result = self.chain2({'context': context, 'query': query})\n",
    "        # 调用问答链并返回结果\n",
    "        # result = self.qa_chain({\"query\": question, \"temperature\": temperature, \"top_k\": top_k})\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f5564b5-9038-49e6-ac3b-d52fb991f8d2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-17T06:31:18.016426Z",
     "iopub.status.busy": "2024-10-17T06:31:18.016091Z",
     "iopub.status.idle": "2024-10-17T06:31:21.815462Z",
     "shell.execute_reply": "2024-10-17T06:31:21.814997Z",
     "shell.execute_reply.started": "2024-10-17T06:31:18.016407Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./AI-ModelScope/tao-8k. Creating a new one with mean pooling.\n",
      "/tmp/ipykernel_374/1405062993.py:85: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  self.chain1 = LLMChain(\n",
      "/tmp/ipykernel_374/1405062993.py:130: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.chain2({'context': context, 'query': query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': '响，我们党面临的执政环境仍然是复杂的，影响党的先进性、弱化党的纯洁性的因素也是\\n复杂的。党的队伍和自身状况发生重大而深刻的变化，迫切要求提高党的建设质量、增强\\n党组织的政治功能和组织功能。我们要坚持问题导向，保持战略定力，以“越是艰险越向\\n前”的英雄气概和“狭路相逢勇者胜”的斗争精神，坚定不移抓下去。\\n习近平指出，要坚持以党的政治建设为统领，坚决维护党中央权威和集中统一领导。404习近平强调，全面从严治党必须持之以恒、毫不动摇。受国际国内环境各种因素的影\\n响，我们党面临的执政环境仍然是复杂的，影响党的先进性、弱化党的纯洁性的因素也是\\n复杂的。党的队伍和自身状况发生重大而深刻的变化，迫切要求提高党的建设质量、增强\\n党组织的政治功能和组织功能。我们要坚持问题导向，保持战略定力，以“越是艰险越向\\n前”的英雄气概和“狭路相逢勇者胜”的斗争精神，坚定不移抓下去。习近平指出，我们党是生于忧患、成长于忧患、壮大于忧患的政党。正是一代代中国\\n共产党人心存忧患、肩扛重担，才团结带领中国人民不断从胜利走向新的胜利。我国形势\\n总的是好的，但我们前进道路上面临的困难和风险也不少。国内外环境发生了深刻变化，\\n面对的矛盾和问题发生了深刻变化，发展阶段和发展任务发生了深刻变化，工作对象和工\\n作条件发生了深刻变化，对我们党长期执政能力和领导水平的要求也发生了深刻变化。中', 'query': '习近平总书记强调，我们党全面领导、长期执政，面临的最大挑战是什么？', 'text': '习近平总书记强调，我们党全面领导、长期执政，面临的最大挑战是党的先进性和纯洁性受到的影响以及党的建设质量、党组织的政治功能和组织功能需要进一步增强的问题。在复杂多变的执政环境下，如何保持党的先进性和纯洁性，提升党的建设质量，增强党组织的政治功能和组织功能，是我们党面临的最大挑战。'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt: str  # 用户 prompt\n",
    "model: str = \"glm-4-flash\"  # 使用的模型\n",
    "temperature: float = 0.1  # 温度系数\n",
    "if_history: bool = False  # 是否使用历史对话功能\n",
    "# API_Key\n",
    "api_key: str = \"652a160546149ef4e3ec0ff881beebfe.D3UaKuk7FmiUn9WQ\"\n",
    "# Secret_Key\n",
    "secret_key: str = None\n",
    "# access_token\n",
    "access_token: str = None\n",
    "# APPID\n",
    "appid: str = None\n",
    "# APISecret\n",
    "Spark_api_secret: str = None\n",
    "# Secret_key\n",
    "Wenxin_secret_key: str = None\n",
    "# 数据库路径\n",
    "db_path: str = \"./vector_db\"\n",
    "# 源文件路径\n",
    "file_path: str = \"./中国近现代史纲要：2023 年版(1).pdf\"\n",
    "# prompt template\n",
    "prompt_template: str = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。最多使用三句话。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。\n",
    "{context}\n",
    "问题: {question}\n",
    "有用的回答:\"\"\"\n",
    "# Template 变量\n",
    "input_variables: list = [\"context\", \"question\"]\n",
    "# Embdding\n",
    "embedding: str = \"m3e\"\n",
    "# Top K\n",
    "top_k: int = 5\n",
    "# embedding_key\n",
    "embedding_key = \"652a160546149ef4e3ec0ff881beebfe.D3UaKuk7FmiUn9WQ\"\n",
    "\n",
    "choiceproblem_file_path=\"./vector_db/choiceproblem\"\n",
    "analysis_vectordb_file_path=\"./vector_db/tao_8k\"\n",
    "embedding_path = \"./AI-ModelScope/tao-8k\"\n",
    "\n",
    "\n",
    "def get_chain():\n",
    "    return QA_chain_self(model=model, temperature=temperature, top_k=top_k,\n",
    "                              choiceproblem_file_path=choiceproblem_file_path, analysis_vectordb_file_path=analysis_vectordb_file_path, persist_path=db_path,\n",
    "                              api_key=api_key, embedding_path=embedding_path, template=prompt_template)\n",
    "\n",
    "\n",
    "response = get_chain().answer(query=\"习近平总书记强调，我们党全面领导、长期执政，面临的最大挑战是什么？\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
