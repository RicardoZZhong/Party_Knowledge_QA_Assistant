{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8764e3e-9d2f-4805-b9d6-415f01ca5955",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"./train_dataset4.json\"\n",
    "with open(path) as f:\n",
    "    datas = json.load(f)\n",
    "    \n",
    "print(type(data))\n",
    "print(datas[0]['query'])\n",
    "\n",
    "query=[]\n",
    "positive=[]\n",
    "for data in datas:\n",
    "    query.append(data['query'])\n",
    "    positive.append(data['answer'])\n",
    "print(query)\n",
    "print(positive)\n",
    "\n",
    "train_data = {}\n",
    "train_data['anchor'] = query\n",
    "train_data['positive'] = positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956c757d-80a0-41dc-a8f9-2f530166e8af",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-14T10:22:45.958748Z",
     "iopub.status.busy": "2024-10-14T10:22:45.958380Z",
     "iopub.status.idle": "2024-10-14T10:22:49.285166Z",
     "shell.execute_reply": "2024-10-14T10:22:49.284626Z",
     "shell.execute_reply.started": "2024-10-14T10:22:45.958728Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./AI-ModelScope/tao-8k. Creating a new one with mean pooling.\n",
      "挖掘困难样本: 100%|██████████| 438/438 [00:02<00:00, 156.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['anchor', 'positive', 'negative'],\n",
      "    num_rows: 438\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import os\n",
    "import random\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class BuildTrainData:\n",
    "    def __init__(self, model_path, train_data, file_name):\n",
    "        # logging.info(\"加载原始数据...\")\n",
    "        self.data = train_data\n",
    "        # logging.info(f\"从 {model_path} 加载向量化模型...\")\n",
    "        self.model = SentenceTransformer(model_path)\n",
    "        self.model.eval()\n",
    "        self.batch_size = 32\n",
    "        self.faiss_measure = faiss.METRIC_L2\n",
    "        self.index_type = \"HNSW64\"\n",
    "\n",
    "        save_dir = \".\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.embedding_path = f\"{save_dir}/embedding_{file_name}.pkl\"\n",
    "        self.faiss_index_path = f\"{save_dir}/faiss_{file_name}.index\"\n",
    "        self.bge_train_data_path = f\"data/{file_name}_hnm_train.jsonl\"\n",
    "\n",
    "    def init_embedding(self, embedding_path):\n",
    "        loader = PyMuPDFLoader(\"\")\n",
    "        pages = loader.load()\n",
    "        text_list=[]\n",
    "        for page in pages:\n",
    "            text_list.append(page.page_content)\n",
    "        embeddings = self.model.encode(text_list, batch_size=self.batch_size, show_progress_bar=True)\n",
    "        joblib.dump(embeddings, self.embedding_path)\n",
    "\n",
    "    #   embedding 方法用于将文本列表转换为向量表示\n",
    "    def embedding(self, text_list):\n",
    "        # logging.info(\"向量化...\")\n",
    "        embeddings = self.model.encode(text_list, batch_size=self.batch_size, show_progress_bar=True)\n",
    "        return embeddings\n",
    "\n",
    "    def embedding_mul_gpu(self, text_list):\n",
    "        logging.info(\"多GPU并行向量化...\")\n",
    "        # 通过target_devices指定GPU，如target_devices=['cuda:0', 'cuda:1']\n",
    "        pool = self.model.start_multi_process_pool()\n",
    "        embeddings = self.model.encode_multi_process(text_list, pool, batch_size=self.batch_size)\n",
    "        self.model.stop_multi_process_pool(pool)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    #  用于构建FAISS索引\n",
    "    def build_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_path):\n",
    "            # logging.info(f\"{self.faiss_index_path}已存在...\")\n",
    "            faiss_index = faiss.read_index(self.faiss_index_path)\n",
    "            embeddings = joblib.load(self.embedding_path)\n",
    "            return faiss_index, embeddings\n",
    "        # logging.info(\"从本地加载向量化的数据...\")\n",
    "        embeddings = joblib.load(self.embedding_path)\n",
    "        dim = embeddings.shape[1]\n",
    "        faiss_index = faiss.index_factory(dim, self.index_type, self.faiss_measure)\n",
    "        # logging.info(\"构建索引...\")\n",
    "        faiss_index.add(embeddings)\n",
    "        faiss.write_index(faiss_index, self.faiss_index_path)\n",
    "        return faiss_index, embeddings\n",
    "\n",
    "    def compute_retrival(self, mul_gpus=None, retrival_topk=100):\n",
    "        # logging.info(\"挖掘困难样本...\")\n",
    "        query_list = self.data[\"anchor\"]\n",
    "\n",
    "        # query = \"为这个句子生成表示以用于检索相关文章：\" + row[\"query\"]\n",
    "        if not os.path.exists(self.embedding_path):\n",
    "            # logging.info(\"embedding 文件不存在, 重新embedding...\")\n",
    "            if not mul_gpus:\n",
    "                # logging.info(\"只使用一个GPU...\")\n",
    "                query_embedding = self.embedding(self.data[\"positive\"])\n",
    "            else:\n",
    "                # logging.info(\"多GPU加速...\")\n",
    "                query_embedding = self.embedding_mul_gpu(self.data[\"positive\"])\n",
    "            joblib.dump(query_embedding, self.embedding_path)\n",
    "        faiss_index, query_embedding = self.build_faiss_index()\n",
    "\n",
    "        # logging.info(\"开始处理数据...\")\n",
    "        distances, indexs = faiss_index.search(query_embedding, retrival_topk)\n",
    "\n",
    "        anchor, positive, negative = [], [], []\n",
    "        for idx, query in enumerate(tqdm(query_list, desc=\"挖掘困难样本\")):\n",
    "            answer = self.data[\"positive\"][idx]\n",
    "            target_answers = []\n",
    "\n",
    "            # dist越小越相似\n",
    "            neg_samples_tune = []\n",
    "            for dist, df_idx in zip(*[distances[idx], indexs[idx]]):\n",
    "                if df_idx == -1:\n",
    "                    # logging.info(f\"bade index {df_idx}\")\n",
    "                    continue\n",
    "\n",
    "                target_query = self.data[\"anchor\"][df_idx]\n",
    "                if target_query == query:\n",
    "                    continue\n",
    "                target_answer = self.data[\"positive\"][df_idx]\n",
    "                if target_answer == answer:\n",
    "                    continue\n",
    "\n",
    "                if dist > 0.4 and dist <= 0.7:\n",
    "                    target_answers.append(target_answer)\n",
    "                elif dist > 0.7:\n",
    "                    neg_samples_tune.append(target_answer)\n",
    "\n",
    "            if len(target_answers) == 0:\n",
    "                \n",
    "                target_answers = neg_samples_tune\n",
    "                if len(target_answers) == 0:\n",
    "                \n",
    "                    continue\n",
    "            elif len(target_answers) > 10:\n",
    "                target_answers = random.sample(target_answers, 10)\n",
    "\n",
    "            anchor.append(query)\n",
    "            positive.append(answer)\n",
    "            negative.append(random.sample(target_answers, 1)[0])\n",
    "            triplet = Dataset.from_dict({\"anchor\": anchor, \"positive\": positive, \"negative\": negative})\n",
    "            \n",
    "#             joblib.dump(triplet, './triplet.pkl')\n",
    "            # 将 Dataset 对象转换为字典\n",
    "            data_dict = triplet.to_dict()\n",
    "\n",
    "            # 保存为 JSON 文件\n",
    "            with open('data.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(data_dict, f, ensure_ascii=False, indent=4)\n",
    "        return triplet\n",
    "\n",
    "\n",
    "\n",
    "path = \"./train_dataset4.json\"\n",
    "with open(path) as f:\n",
    "    datas = json.load(f)\n",
    "\n",
    "query = []\n",
    "positive = []\n",
    "for data in datas:\n",
    "    query.append(data['query'])\n",
    "    positive.append(data['answer'])\n",
    "    \n",
    "train_data = {}\n",
    "train_data['anchor'] = query\n",
    "train_data['positive'] = positive\n",
    "btd = BuildTrainData(\"./AI-ModelScope/tao-8k\", train_data, \"test\")\n",
    "triplet = btd.compute_retrival()\n",
    "\n",
    "print(triplet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
