{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f0301-5e44-43c9-8007-d33a561fe94d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T02:27:48.563242Z",
     "iopub.status.busy": "2024-09-13T02:27:48.562949Z",
     "iopub.status.idle": "2024-09-13T02:28:33.356656Z",
     "shell.execute_reply": "2024-09-13T02:28:33.356156Z",
     "shell.execute_reply.started": "2024-09-13T02:27:48.563223Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading [1_Pooling/config.json]: 100%|██████████| 190/190 [00:00<00:00, 422B/s]\n",
      "Downloading [config.json]: 100%|██████████| 776/776 [00:00<00:00, 1.42kB/s]\n",
      "Downloading [config_sentence_transformers.json]: 100%|██████████| 124/124 [00:00<00:00, 226B/s]\n",
      "Downloading [configuration.json]: 100%|██████████| 47.0/47.0 [00:00<00:00, 87.1B/s]\n",
      "Downloading [model.safetensors]: 100%|██████████| 91.4M/91.4M [00:00<00:00, 112MB/s] \n",
      "Downloading [modules.json]: 100%|██████████| 349/349 [00:00<00:00, 635B/s]\n",
      "Downloading [pytorch_model.bin]: 100%|██████████| 91.4M/91.4M [00:00<00:00, 101MB/s] \n",
      "Downloading [README.md]: 100%|██████████| 27.5k/27.5k [00:00<00:00, 48.2kB/s]\n",
      "Downloading [sentence_bert_config.json]: 100%|██████████| 52.0/52.0 [00:00<00:00, 101B/s]\n",
      "Downloading [special_tokens_map.json]: 100%|██████████| 125/125 [00:00<00:00, 190B/s]\n",
      "Downloading [tokenizer.json]: 100%|██████████| 429k/429k [00:00<00:00, 678kB/s]\n",
      "Downloading [tokenizer_config.json]: 100%|██████████| 367/367 [00:00<00:00, 666B/s]\n",
      "Downloading [vocab.txt]: 100%|██████████| 107k/107k [00:00<00:00, 158kB/s]\n",
      "Downloading [config.json]: 100%|██████████| 0.98k/0.98k [00:00<00:00, 1.56kB/s]\n",
      "Downloading [config_cpu.json]: 100%|██████████| 0.98k/0.98k [00:00<00:00, 1.95kB/s]\n",
      "Downloading [configuration.json]: 100%|██████████| 39.0/39.0 [00:00<00:00, 72.4B/s]\n",
      "Downloading [configuration_yuan.py]: 100%|██████████| 1.29k/1.29k [00:00<00:00, 2.44kB/s]\n",
      "Downloading [generation_config.json]: 100%|██████████| 144/144 [00:00<00:00, 333B/s]\n",
      "Downloading [LICENSE]: 100%|██████████| 7.86k/7.86k [00:00<00:00, 17.6kB/s]\n",
      "Downloading [pytorch_model.bin]: 100%|██████████| 4.41G/4.41G [00:14<00:00, 326MB/s] \n",
      "Downloading [README.md]: 100%|██████████| 7.61k/7.61k [00:00<00:00, 18.3kB/s]\n",
      "Downloading [special_tokens_map.json]: 100%|██████████| 411/411 [00:00<00:00, 731B/s]\n",
      "Downloading [tokenizer.model]: 100%|██████████| 2.06M/2.06M [00:00<00:00, 3.98MB/s]\n",
      "Downloading [tokenizer_config.json]: 100%|██████████| 1.12k/1.12k [00:00<00:00, 1.67kB/s]\n",
      "Downloading [yuan_hf_model.py]: 100%|██████████| 52.0k/52.0k [00:00<00:00, 90.3kB/s]\n",
      "Downloading [yuan_hf_model_cpu.py]: 100%|██████████| 52.0k/52.0k [00:00<00:00, 95.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 向量模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir='./')\n",
    "\n",
    "# 源大模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3174eee-3b7a-4b58-a4aa-68c4d270af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型路径\n",
    "model_path = './IEITYuan/Yuan2-2B-Mars-hf'\n",
    "\n",
    "# 定义向量模型路径\n",
    "embedding_model_path = './AI-ModelScope/bge-small-zh-v1___5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2232e-ee1b-458a-af3e-ef7cfc772af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "from typing import Any, List, Optional\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 定义源大模型类\n",
    "class Yuan2_LLM(LLM):\n",
    "    \"\"\"\n",
    "    class for Yuan2_LLM\n",
    "    \"\"\"\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, mode_path :str):\n",
    "        super().__init__()\n",
    "\n",
    "        # 加载预训练的分词器和模型\n",
    "        print(\"Creat tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(mode_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)\n",
    "\n",
    "        print(\"Creat model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(mode_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        prompt = prompt.strip()\n",
    "        prompt += \"<sep>\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "        outputs = self.model.generate(inputs,do_sample=False,max_length=4096)\n",
    "        output = self.tokenizer.decode(outputs[0])\n",
    "        response = output.split(\"<sep>\")[-1].split(\"<eod>\")[0]\n",
    "\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Yuan2_LLM\"\n",
    "\n",
    "# 定义一个函数，用于获取llm和embeddings\n",
    "@st.cache_resource\n",
    "def get_models():\n",
    "    llm = Yuan2_LLM(model_path)\n",
    "\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    return llm, embeddings\n",
    "\n",
    "template = \"\"\"使用以下上下文片段来回答最后的问题。不要试图编造答案。不要重复回答。尽量简明扼要地回答。\n",
    "{context}\n",
    "问题：{query}\"\"\"\n",
    "\n",
    "# 定义ChatBot类\n",
    "class ChatBot:\n",
    "    \"\"\"\n",
    "    class for ChatBot.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, embeddings, vectordb):\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=template\n",
    "        )\n",
    "        self.chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.prompt}\n",
    "        )\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        # 加载 text_splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=450,\n",
    "            chunk_overlap=10,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "    def run(self, query):\n",
    "\n",
    "        sim_docs = vectordb.max_marginal_relevance_search(query, k=5, fetch_k=3)\n",
    "        context = \"\"\n",
    "        for i, sim_doc in enumerate(sim_docs):\n",
    "            context = context + str(i + 1) + \".\" + sim_doc.page_content\n",
    "        self.prompt.format(context=context, query=query)\n",
    "        # # 切分成chunks\n",
    "        # all_chunks = self.text_splitter.split_text(text=text)\n",
    "\n",
    "        # # 转成向量并存储\n",
    "        # VectorStore = FAISS.from_texts(all_chunks, embedding=self.embeddings)\n",
    "\n",
    "        # 检索相似的chunks\n",
    "        # chunks = VectorStore.similarity_search(query=query, k=1)\n",
    "\n",
    "        # 生成回复\n",
    "        response = self.chain.run(question=query)\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 获取llm和embeddings\n",
    "    llm, embeddings = get_models()\n",
    "    # 将csv文件转向量储存\n",
    "    persist_directory = \"./vector_db\"\n",
    "    csvloader = CSVLoader(file_path=\"./my_file.csv\", encoding=\"utf-8\", csv_args={'delimiter': ',', 'quotechar': '\"'})\n",
    "    pdfloader = PyPDFLoader(\"./教材：中国近现代史纲要（2015年版）.pdf\")\n",
    "    pdf = []\n",
    "    # for loader in loaders_chinese:\n",
    "    #     docs.extend(loader.load())\n",
    "    pdf = pdfloader.load()\n",
    "    CHUNK_SIZE = 200\n",
    "    # 知识库中相邻文本重合长度\n",
    "    OVERLAP_SIZE = 70\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=OVERLAP_SIZE\n",
    "    )\n",
    "    split_pdf = text_splitter.split_documents(pdf)\n",
    "    csv = csvloader.load()\n",
    "    pdf = pdfloader.load()\n",
    "    choiceproblem_vectordb = Chroma.from_documents(\n",
    "        documents=csv,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory + \"/choiceproblem\"  # 允许我们将persist_directory目录保存到磁盘上\n",
    "    )\n",
    "    choiceproblem_vectordb.persist()\n",
    "    analysis_vectordb = Chroma.from_documents(\n",
    "        documents=split_pdf,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory + \"/pdf\"  # 允许我们将persist_directory目录保存到磁盘上\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    query = \"在新民主主义的经济纲领中，极具特色的一项内容是什么？\"\n",
    "\n",
    "    # 初始化ChatBot\n",
    "    chatbot = ChatBot(llm, embeddings, vectordb)\n",
    "    \n",
    "    response = chatbot.run(query)\n",
    "    # 生成概括\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
