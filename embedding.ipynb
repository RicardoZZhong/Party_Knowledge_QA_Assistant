{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937dfa79-6f6a-41cf-8523-c70188d900f8",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-09-13T11:11:58.955540Z",
     "iopub.status.busy": "2024-09-13T11:11:58.955217Z",
     "iopub.status.idle": "2024-09-13T11:11:58.958371Z",
     "shell.execute_reply": "2024-09-13T11:11:58.957812Z",
     "shell.execute_reply.started": "2024-09-13T11:11:58.955521Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义模型路径\n",
    "model_path = './IEITYuan/Yuan2-2B-Mars-hf'\n",
    "# model_path = './qwen/Qwen1___5-4B-Chat-GGUF'\n",
    "# 定义向量模型路径\n",
    "embedding_model_path = './AI-ModelScope/bge-large-zh-v1___5'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb3aa65-0e23-4a2a-a94c-ee5de42996bb",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-09-13T11:11:59.942491Z",
     "iopub.status.busy": "2024-09-13T11:11:59.942162Z",
     "iopub.status.idle": "2024-09-13T11:12:02.382525Z",
     "shell.execute_reply": "2024-09-13T11:12:02.381972Z",
     "shell.execute_reply.started": "2024-09-13T11:11:59.942471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "from typing import Any, List, Optional\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import torch\n",
    "# 定义源大模型类\n",
    "class Yuan2_LLM(LLM):\n",
    "    \"\"\"\n",
    "    class for Yuan2_LLM\n",
    "    \"\"\"\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, mode_path :str):\n",
    "        super().__init__()\n",
    "\n",
    "        # 加载预训练的分词器和模型\n",
    "        print(\"Creat tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(mode_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)\n",
    "\n",
    "        print(\"Creat model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(mode_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        prompt = prompt.strip()\n",
    "        prompt += \"<sep>\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "        outputs = self.model.generate(inputs,do_sample=False,max_length=4096)\n",
    "        output = self.tokenizer.decode(outputs[0])\n",
    "        response = output.split(\"<sep>\")[-1].split(\"<eod>\")[0]\n",
    "\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Yuan2_LLM\"\n",
    "\n",
    "# 定义一个函数，用于获取llm和embeddings\n",
    "@st.cache_resource\n",
    "def get_models():\n",
    "    # llm = Yuan2_LLM(model_path)\n",
    "\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafdeec-182a-4a15-a8e4-8d9491e311ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZhipuAILLM(LLM):\n",
    "        # 默认选用 glm-4 模型\n",
    "    model: str = \"glm-4\"\n",
    "    # 温度系数\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"c33e39bf88d7c169484bcd28af694e0f.6XKKA71BsvhG8gNU\"\n",
    "    max_tokens: int = 2048\n",
    "\n",
    "    # 定义 _call 方法：\n",
    "    # 这个方法实现了实际的 API 调用逻辑：\n",
    "    # 初始化 ZhipuAI 客户端。\n",
    "    # 生成请求参数。\n",
    "    # 调用 chat.completions.create 方法获取响应。\n",
    "    # 返回响应中的内容，如果没有结果则返回错误信息。\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # 生成 GLM 模型请求参数的方法：\n",
    "        # 生成 GLM 模型的请求参数 messages，包括系统消息和用户输入\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            构造 GLM 模型请求参数 messages\n",
    "            请求参数：\n",
    "                prompt: 对应的用户提示词\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "\n",
    "        client = ZhipuAI(\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "\n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=messages,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "    # 定义属性方法：\n",
    "    # _default_params：返回调用 API 的默认参数。\n",
    "    # _llm_type：返回模型类型的字符串标识。\n",
    "    # _identifying_params：返回模型的标识参数。\n",
    "    # 首先定义一个返回默认参数的方法\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用Ennie API的默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e1c1ba6-c310-4643-91d8-e20b1e5784d0",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-09-13T11:43:30.423799Z",
     "iopub.status.busy": "2024-09-13T11:43:30.423471Z",
     "iopub.status.idle": "2024-09-13T11:44:52.010668Z",
     "shell.execute_reply": "2024-09-13T11:44:52.010145Z",
     "shell.execute_reply.started": "2024-09-13T11:43:30.423780Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creat tokenizer...\n",
      "Creat model...\n",
      "860\n"
     ]
    }
   ],
   "source": [
    "# 获取llm和embeddings\n",
    "embeddings = get_models()\n",
    "# 将csv文件转向量储存\n",
    "persist_directory = \"./vector_db\"\n",
    "csvloader = CSVLoader(file_path=\"./chioceproblem.csv\", encoding=\"utf-8\", csv_args={'delimiter': ',', 'quotechar': '\"'})\n",
    "pdfloader = PyPDFLoader(\"./中国近现代史纲要：2023 年版(1).pdf\")\n",
    "pdf = []\n",
    "# for loader in loaders_chinese:\n",
    "#     docs.extend(loader.load())\n",
    "pdf = pdfloader.load()\n",
    "CHUNK_SIZE = 500\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "split_pdf = text_splitter.split_documents(pdf)\n",
    "print(len(split_pdf))\n",
    "csv = csvloader.load()\n",
    "choiceproblem_vectordb = Chroma.from_documents(\n",
    "    documents=csv,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory + \"/choiceproblem\"\n",
    ")\n",
    "choiceproblem_vectordb.persist()\n",
    "analysis_vectordb = Chroma.from_documents(\n",
    "    documents=split_pdf,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory + \"/pdf\"\n",
    ")\n",
    "analysis_vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b40a7485-0021-4217-80c8-46ec1a005c7e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-13T11:56:55.517465Z",
     "iopub.status.busy": "2024-09-13T11:56:55.517134Z",
     "iopub.status.idle": "2024-09-13T11:56:55.522827Z",
     "shell.execute_reply": "2024-09-13T11:56:55.522142Z",
     "shell.execute_reply.started": "2024-09-13T11:56:55.517444Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.sequential import SequentialChain\n",
    "\n",
    "template = \"\"\"使用以下背景知识来回答最后的问题。不要试图编造答案。尽量简明扼要地回答。\n",
    "背景知识：现在有这样一道题目：{context}，\n",
    "问题：{query}\"\"\"\n",
    "\n",
    "\n",
    "template1 = \"\"\"给你一个任务：给定一段文本，这段文本中第一行是题目，题目中会有括号，后面四行为A，B，C，D四个选项，最后一行是题目的答案。你需要做的是：将题目中的括号以及括号内的内容替换为答案并输出。\n",
    "不需要你检查指出题目的对错，不要输出除了以上任务之外的任何回答，否则地球会爆炸！\n",
    "这是你需要处理的文本：{question}\n",
    "\"\"\"\n",
    "\n",
    "# 定义ChatBot类\n",
    "class ChatBot:\n",
    "    \"\"\"\n",
    "    class for ChatBot.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, embeddings, choiceproblem_vectordb, analysis_vectordb):\n",
    "        self.prompt1 = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=template1\n",
    "        )\n",
    "        self.chain1 = LLMChain(\n",
    "            llm=llm, prompt=self.prompt1\n",
    "        )\n",
    "        self.embeddings = embeddings\n",
    "        self.choiceproblem_vectordb = choiceproblem_vectordb\n",
    "        self.analysis_vectordb = analysis_vectordb\n",
    "        # self.overall_chain = SequentialChain(\n",
    "        #     chains=[self.prompt1],\n",
    "        #     verbose=True,\n",
    "        #     input_variables=[\"question\"],\n",
    "        #     output_variables=[\"right_answer\"]\n",
    "        # )\n",
    "\n",
    "    def run(self, query):\n",
    "\n",
    "        sim_docs = self.choiceproblem_vectordb.max_marginal_relevance_search(query, k=1, fetch_k=1)\n",
    "        question = \"\"\n",
    "        for i, sim_doc in enumerate(sim_docs):\n",
    "            question = question + sim_doc.page_content+'\\n'\n",
    "        # related_analysis = self.analysis_vectordb.max_marginal_relevance_search(query, k=1, fetch_k=1)\n",
    "        # for i, related_doc in enumerate(related_analysis):\n",
    "        #     context = context + related_doc.page_content\n",
    "        # print(context)\n",
    "        # self.prompt.format(context=context, query=query)\n",
    "        # 生成回复\n",
    "        response = self.chain1(question)\n",
    "        # res\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a01f82bd-6242-46c3-b165-3cbb883d757b",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-09-13T01:58:02.129207Z",
     "iopub.status.busy": "2024-09-13T01:58:02.128843Z",
     "iopub.status.idle": "2024-09-13T01:58:02.860602Z",
     "shell.execute_reply": "2024-09-13T01:58:02.859856Z",
     "shell.execute_reply.started": "2024-09-13T01:58:02.129186Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# 向量模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('qwen/Qwen1.5-4B-Chat-GGUF', cache_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "223d959d-d243-491e-9fc0-d5a5830c9d2f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-13T11:56:57.896889Z",
     "iopub.status.busy": "2024-09-13T11:56:57.896517Z",
     "iopub.status.idle": "2024-09-13T11:57:01.836246Z",
     "shell.execute_reply": "2024-09-13T11:57:01.835594Z",
     "shell.execute_reply.started": "2024-09-13T11:56:57.896867Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: 235、党在新时代的强军目标是建设一支(   )的人民军队。\n",
      "A: 听党指挥，能打能杀，坚定执着\n",
      "B: 作风优良，敢打硬仗，不怕吃苦\n",
      "C: 强大后勤保障\n",
      "D: 听党指挥、能打胜仗、作风优良\n",
      "答案: 听党指挥、能打胜仗、作风优良\n",
      "\n",
      "--------\n",
      " 问题: 235、党在新时代的强军目标是建设一支(   )的人民军队。\n",
      "A: 听党指挥，能打能杀，坚定执着\n",
      "B: 作风优良，敢打硬仗，不怕吃苦\n",
      "C: 强大后勤保障\n",
      "D: 听党指挥、能打胜仗、作风优良\n",
      "答案: 听党指挥、能打胜仗、作风优良\n"
     ]
    }
   ],
   "source": [
    "query = \"党在新时代的强军目标是建设一支(   )的人民军队\"\n",
    "\n",
    "# 初始化ChatBot\n",
    "chatbot = ChatBot(llm, embeddings, choiceproblem_vectordb, analysis_vectordb)\n",
    "\n",
    "response = chatbot.run(query)\n",
    "print(response['question'])\n",
    "print(\"--------\")\n",
    "print(response['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
